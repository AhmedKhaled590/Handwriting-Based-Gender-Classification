{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from cmath import e\n",
    "from time import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from skimage.measure import shannon_entropy as Entropy\n",
    "from sklearn import preprocessing, svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,VotingClassifier,BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,cross_validate\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from tuning import svmTuner\n",
    "import utils\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    extractGLCM(filename, outputFileName):\n",
    "    - filename: path to the image\n",
    "    - outputFileName: name of the output file\n",
    "    - returns: numpy array of features\n",
    "\"\"\"\n",
    "def extractGLCM(filename, outputFileName):\n",
    "    img = cv2.imread(filename)\n",
    "    \n",
    "    # Extract Gray Level Channel\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # crop image to remove the bottom part of the image \n",
    "\n",
    "    img = cv2.resize(img,(int(img.shape[1]/2),int(img.shape[0]/2)),interpolation=cv2.INTER_AREA)\n",
    "    img = cv2.GaussianBlur(img,(5,5),0)\n",
    "\n",
    "    from LBP.commonfunctions import show_images\n",
    "    # show_images([img])\n",
    "    \n",
    "    \n",
    "    \n",
    "    step = [1]  # step size\n",
    "    step = np.asarray(step)\n",
    "    angle = [0, np.pi/4, np.pi/2, 3*np.pi/4]  # angles (0, 45, 90, 135)\n",
    "\n",
    "    coOccuranceMat = graycomatrix(\n",
    "        img, step, angle, levels=256, normed=True)\n",
    "\n",
    "    # calculate the GLCM properties\n",
    "    contrast = graycoprops(coOccuranceMat, prop='contrast')    \n",
    "    correlation = graycoprops(coOccuranceMat, prop='correlation')\n",
    "    energy = graycoprops(coOccuranceMat, prop='energy')\n",
    "    homogeneity = graycoprops(coOccuranceMat, prop='homogeneity')\n",
    "    # ASM = graycoprops(coOccuranceMat, prop='ASM')\n",
    "    \n",
    "    # entropy = []\n",
    "    # entropy.insert(0, Entropy(coOccuranceMat[0, 0, :, :]))\n",
    "    # entropy.insert(1, Entropy(coOccuranceMat[0, 1, :, :]))\n",
    "    # entropy.insert(2, Entropy(coOccuranceMat[1, 0, :, :]))\n",
    "    # entropy.insert(3, Entropy(coOccuranceMat[1, 1, :, :]))\n",
    "    # entropy = np.array(entropy)\n",
    "\n",
    "    # calculate Entropy\n",
    "    # entropy(i,j) = -sum(p(i,j) * log(p(i,j)))\n",
    "    entropy = -np.sum(coOccuranceMat * np.log(coOccuranceMat + 1e-100), axis=(0, 1))\n",
    "    \n",
    "\n",
    "    \n",
    "    # append all features to a numpy array\n",
    "    features = np.array([contrast.flatten(), correlation.flatten(),\n",
    "                        homogeneity.flatten(), entropy.flatten(), energy.flatten()])\n",
    "\n",
    "    features = features.flatten()\n",
    "    features = features.reshape(1, -1)\n",
    "    \n",
    "\n",
    "    with open(outputFileName+'.csv', 'a') as csvfile:\n",
    "        np.savetxt(csvfile, features, fmt='%f', delimiter=',')\n",
    "        csvfile.close()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractGLCM('Females/Females/F5.jpg','d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeFileIfExists(fileName):\n",
    "    if os.path.isfile(fileName):\n",
    "        os.remove(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeFeaturesToFile(features, fileName):\n",
    "    with open(fileName, 'a') as csvfile:\n",
    "        np.savetxt(csvfile, features, fmt='%f', delimiter=',')\n",
    "        csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeHeadersOfCSVFile(fileName):\n",
    "    with open(fileName, 'a') as csvfile:\n",
    "        np.savetxt(csvfile, [], delimiter=',',\n",
    "                   header='Contrast1,Contrast2,Contrast3,Contrast4,homogeneity1,homogeneity2,homogeneity3,homogeneity4,energy1,energy2,energy3,energy4,correlation1,correlation2,correlation3,correlation4,entropy1,entropy2,entropy3,entropy4')\n",
    "        csvfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFeaturesFromFile(fileName):\n",
    "    CSVData = open(fileName)\n",
    "    features = np.genfromtxt(CSVData, delimiter=\",\")\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeaturesFromFolder(folder,outputFileName,gender):\n",
    "    train_classes=[]\n",
    "    features=[]\n",
    "    for filename in os.listdir(folder):\n",
    "        try:\n",
    "            features.append(extractGLCM(folder+filename,outputFileName))\n",
    "            train_classes.append(gender)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    return np.array(features),np.array(train_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractICDARFeatures():\n",
    "    features=[]\n",
    "    # read csv file\n",
    "    df = pd.read_csv('train_answers.csv')\n",
    "    # get the labels\n",
    "    icdar_classes = df['male'].values\n",
    "    print(icdar_classes.shape)\n",
    "    icdar_classes_train = np.array([])\n",
    "    i = 0\n",
    "    for filename in os.listdir('images_gender/images/train'):\n",
    "        try:\n",
    "            features.append(extractGLCM('images_gender/images/train/'+filename,'icdar'))\n",
    "            icdar_classes_train = np.append(icdar_classes_train, icdar_classes[i//2])\n",
    "            i = i + 1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    icdar_classes  = icdar_classes_train\n",
    "    return np.array(features),np.array(icdar_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestParamsForANN(X_train,Y_train,X_test,Y_test):\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    solver = ['adam']\n",
    "    alpha = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "    max_iter = [1000, 2000, 3000, 4000]\n",
    "    layer_sizes = [(5,),(5,5),(15,),(10,10)]\n",
    "    scores = []\n",
    "    scores_train = []\n",
    "    with open('ann.csv','w') as csvfile:\n",
    "        np.savetxt(csvfile, [], delimiter=',',\n",
    "                   header='solver,alpha,max_iter,layer_size,score_train,score')\n",
    "    for i in range(len(solver)):\n",
    "        for j in range(len(alpha)):\n",
    "            for k in range(len(max_iter)):\n",
    "                for l in range(len(layer_sizes)):\n",
    "                    clf = MLPClassifier(solver=solver[i], alpha=alpha[j], max_iter=max_iter[k],\n",
    "                            hidden_layer_sizes=(layer_sizes[l]),random_state=1)\n",
    "                    clf.fit(scaler.transform(X_train), Y_train)\n",
    "                    temp_train = clf.score(scaler.transform(X_train), Y_train)\n",
    "                    scores_train.append((temp_train,i,j,k,l))\n",
    "                    print(\"Accuracy on training set: {:.2f}\".format(\n",
    "                        temp_train))\n",
    "                    temp = clf.score(scaler.transform(X_test), Y_test)\n",
    "                    scores.append((temp,i,j,k,l))\n",
    "                    print(\"Accuracy on test set: {:.4f}\".format(\n",
    "                        temp))\n",
    "                    with open('ann.csv','a') as csvfile:\n",
    "                        np.savetxt(csvfile, np.array([[solver[i],alpha[j],max_iter[k],layer_sizes[l],temp_train,temp]]), delimiter=',',fmt=\"%s\")\n",
    "                    print(\"\\n\")  \n",
    "    \n",
    "    # print max score and score_training together with params\n",
    "    print(\"\\n\")\n",
    "    print(max(scores))\n",
    "    print((max(scores_train)))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestParamsForSVM(X_train, Y_train, scalerOutputFileName='scaler.joblib'):\n",
    "    scaler = preprocessing.MinMaxScaler().fit(X_train)\n",
    "\n",
    "    GridSearchCV_parameters = {\n",
    "        'C': [0.01, 0.1, 1, 10, 100, 1000],\n",
    "        'gamma': [0.0001, 0.001, 0.01, 0.1, 1, 'scale'],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    }\n",
    "\n",
    "    t0 = time()\n",
    "    clf = GridSearchCV(SVC(class_weight='balanced'),\n",
    "                       GridSearchCV_parameters,  refit=True, cv=9)\n",
    "\n",
    "    clf = clf.fit(scaler.transform(X_train), Y_train)\n",
    "    print(\"Best estimator found by grid search:\")\n",
    "    print(\"\\nBest parameters: \", clf.best_params_)\n",
    "    print(\"Mean Cross Validation Score: %0.2f\" % clf.best_score_)\n",
    "    print(\"Training time: %.3f\" % (time() - t0))\n",
    "    return clf.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeaturesFromScratch():\n",
    "    removeFileIfExists('female.csv')\n",
    "    removeFileIfExists('male.csv')\n",
    "    removeFileIfExists('icdar.csv')\n",
    "\n",
    "    writeHeadersOfCSVFile('female.csv')\n",
    "    writeHeadersOfCSVFile('male.csv')\n",
    "    writeHeadersOfCSVFile('icdar.csv')\n",
    "\n",
    "    f_features,f_classes = extractFeaturesFromFolder('Females/Females/','female',0)\n",
    "    f_features =  f_features.reshape(f_features.shape[0], -1)\n",
    "    print(f_features.shape)\n",
    "    print(f_classes.shape)\n",
    "    m_features,m_classes = extractFeaturesFromFolder('Males/Males/','male',1)\n",
    "    m_features =  m_features.reshape(m_features.shape[0], -1)\n",
    "    print(m_features.shape)\n",
    "    print(m_classes.shape)\n",
    "    i_features,i_classes = extractICDARFeatures()\n",
    "    i_features = i_features.reshape(i_features.shape[0], -1)\n",
    "    print(i_features.shape)\n",
    "    print(i_classes.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestParamsForRandomForest(X_train,Y_train,X_test,Y_test):\n",
    "    n_estimators = [10,50,100,200]\n",
    "    max_depth = [2,5,10,20,50]\n",
    "    min_samples_split = [2,5,10,20,50]\n",
    "    min_samples_leaf = [1,2,5,10,20]\n",
    "    max_features = ['auto']\n",
    "    bootstrap = [True,False]\n",
    "    scores = []\n",
    "    scores_train = []\n",
    "    with open('randomForest0.csv', 'w') as csvfile:\n",
    "        np.savetxt(csvfile,[],\n",
    "                   header='n_estimators,max_depth,min_samples_split,min_samples_leaf,max_features,bootstrap,score_training,score',\n",
    "                   delimiter=',', fmt='%s')\n",
    "    for i in range(len(n_estimators)):\n",
    "        for j in range(len(max_depth)):\n",
    "            for k in range(len(min_samples_split)):\n",
    "                for l in range(len(min_samples_leaf)):\n",
    "                    for m in range(len(max_features)):\n",
    "                        for n in range(len(bootstrap)):\n",
    "                                clf = RandomForestClassifier(n_estimators=n_estimators[i],max_depth=max_depth[j],min_samples_split=min_samples_split[k],\n",
    "                                                             min_samples_leaf=min_samples_leaf[l],max_features=max_features[m],bootstrap=bootstrap[n],\n",
    "                                                             criterion='gini',random_state=1)\n",
    "                                clf.fit(X_train, Y_train)\n",
    "                                temp_train = clf.score(X_train, Y_train)\n",
    "                                scores_train.append((temp_train,i,j,k,l,m,n))\n",
    "                                print(\"Accuracy on training set: {:.2f}\".format(\n",
    "                                    temp_train))\n",
    "                                temp = clf.score( X_test, Y_test)\n",
    "                                scores.append((temp,i,j,k,l,m,n))\n",
    "                                print(\"Accuracy on test set: {:.4f}\".format(\n",
    "                                    temp))\n",
    "                                print(\"\\n\")\n",
    "                                with open('randomForest0.csv', 'a') as csvFile:\n",
    "                                    np.savetxt(csvFile, np.array([[n_estimators[i],max_depth[j],min_samples_split[k],min_samples_leaf[l],max_features[m],bootstrap[n],temp_train,temp]]), delimiter=',', fmt='%s')\n",
    "    print(max(scores))\n",
    "    print(max(scores_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForest(X_train,Y_train,X_test,Y_test):\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    clf = RandomForestClassifier(n_estimators=10,max_depth=5,min_samples_split=10,min_samples_leaf=1,bootstrap=False,max_features='log2',criterion='gini',random_state=1)\n",
    "    clf.fit(scaler.transform(X_train), Y_train)\n",
    "    # score = clf.score(X_test, Y_test)\n",
    "    predicted_labels = clf.predict(scaler.transform(X_test))\n",
    "    score = accuracy_score(Y_test, predicted_labels)\n",
    "    print(\"Accuracy on test set: {:.4f}\".format(score))\n",
    "    print(\"Accuracy on training set: {:.4f}\".format(clf.score(scaler.transform( X_train), Y_train)))\n",
    "    return predicted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(X_train,Y_train,X_test,Y_test):\n",
    "    # train the classifier and predict the test data\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    print(\"Training the classifier...\")          \n",
    "    clf = SVC(C=100.0, gamma=0.01,kernel='rbf',class_weight='balanced',random_state=1)\n",
    "    clf.fit(scaler.transform( X_train), Y_train) \n",
    "    \n",
    "    print(\"Predicting the test data...\")\n",
    "    # score_training = clf.score(scaler.transform( X_train), Y_train) \n",
    "    # score = clf.score(scaler.transform(X_test), Y_test)\n",
    "    predicted_labels = clf.predict(scaler.transform(X_test))\n",
    "    score = accuracy_score(Y_test, predicted_labels)\n",
    "    print(\"Accuracy on test set: {:.4f}\".format(score))\n",
    "    print(\"Accuracy on training set: {:.4f}\".format(clf.score(scaler.transform( X_train), Y_train)))\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTree(X_train,Y_train,X_test,Y_test):\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    clf = DecisionTreeClassifier(random_state=1,max_depth=5,min_samples_split=50,min_samples_leaf=10,max_features='auto',criterion='gini')\n",
    "    clf.fit(scaler.transform(X_train), Y_train)\n",
    "    # score = clf.score(X_test, Y_test)\n",
    "    predicted_labels = clf.predict(scaler.transform(X_test))\n",
    "    score = accuracy_score(Y_test, predicted_labels)\n",
    "    print(\"Accuracy on test set: {:.4f}\".format(score))\n",
    "    print(\"Accuracy on training set: {:.4f}\".format(clf.score(scaler.transform(X_train), Y_train)))\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidation(X_train,Y_train):\n",
    "    # X_train,Y_train = shuffle(X_train,Y_train)\n",
    "    scores = cross_validate(RandomForestClassifier(n_estimators=50,max_depth=100,min_samples_split=20,min_samples_leaf=5,bootstrap=True,max_features='log2',criterion='gini',random_state=1), X_train, Y_train, cv=5,return_train_score=True)\n",
    "    print(sorted(scores.keys()))\n",
    "    print(scores['test_score'])\n",
    "    print(scores['train_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestParamsForDT(X_train,Y_train,X_test,Y_test):\n",
    "    max_depth = [2,5,10,20,50,100]\n",
    "    min_samples_split = [2,5,10,20,50]\n",
    "    min_samples_leaf = [1,2,5,10,20]\n",
    "    max_features = ['auto','sqrt','log2']\n",
    "    scores = []\n",
    "    scores_train = []\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    with open('decisionTree.csv', 'w') as csvfile:\n",
    "        np.savetxt(csvfile,[],\n",
    "                   header='max_depth,min_samples_split,min_samples_leaf,max_features,score_training,score',\n",
    "                   delimiter=',', fmt='%s')\n",
    "    for i in range(len(max_depth)):\n",
    "        for j in range(len(min_samples_split)):\n",
    "            for k in range(len(min_samples_leaf)):\n",
    "                for l in range(len(max_features)):\n",
    "                        clf = DecisionTreeClassifier(random_state=1,max_depth=max_depth[i],min_samples_split=min_samples_split[j],\n",
    "                                                     min_samples_leaf=min_samples_leaf[k],max_features=max_features[l],criterion='gini')\n",
    "                        clf.fit(scaler.transform(X_train), Y_train)\n",
    "                        temp_train = clf.score(scaler.transform( X_train), Y_train)\n",
    "                        scores_train.append((temp_train,i,j,k,l))\n",
    "                        print(\"Accuracy on training set: {:.2f}\".format(\n",
    "                            temp_train))\n",
    "                        temp = clf.score(scaler.transform( X_test), Y_test)\n",
    "                        scores.append((temp,i,j,k,l))\n",
    "                        print(\"Accuracy on test set: {:.4f}\".format(\n",
    "                            temp))\n",
    "                        print(\"\\n\")\n",
    "                        with open('decisionTree.csv', 'a') as csvFile:\n",
    "                            np.savetxt(csvFile, np.array([[max_depth[i],min_samples_split[j],min_samples_leaf[k],max_features[l],temp_train,temp]]), delimiter=',', fmt='%s')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeaturesFromScratch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(232, 20)\n",
      "(282,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# FeaturesFromScratch() \n",
    "f_features = readFeaturesFromFile('female.csv')\n",
    "m_features = readFeaturesFromFile('male.csv')\n",
    "i_features = readFeaturesFromFile('icdar.csv')\n",
    "\n",
    "\n",
    "# f_features = np.delete(f_features,np.arange(13,17),1)\n",
    "# m_features = np.delete(m_features,np.arange(13,17),1)\n",
    "# i_features = np.delete(i_features,np.arange(13,17),1)\n",
    "\n",
    "# f_features = f_features[:,[0,4,8,12]]\n",
    "# m_features = m_features[:,[0,4,8,12]]\n",
    "# i_features = i_features[:,[0,4,8,12]]\n",
    "\n",
    "\n",
    "print(m_features.shape)\n",
    "\n",
    "train_classes = []\n",
    "# read csv file\n",
    "df = pd.read_csv('train_answers.csv')\n",
    "# get the labels\n",
    "icdar_classes = df['male'].values\n",
    "print(icdar_classes.shape)\n",
    "icdar_classes_train = np.array([])\n",
    "\n",
    "for i in range(1, 132):\n",
    "    try:\n",
    "        train_classes.append(0)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "for i in range(1, 233):\n",
    "    try:\n",
    "        train_classes.append(1)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "for i in range(0, 564):\n",
    "    try:\n",
    "        icdar_classes_train = np.append(icdar_classes_train, icdar_classes[i//2])\n",
    "        i = i + 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "icdar_classes = icdar_classes_train\n",
    "\n",
    "X_train = np.concatenate((f_features,m_features,i_features),axis=0)\n",
    "Y_train = np.concatenate((train_classes,icdar_classes),axis=0)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_train, Y_train, test_size=.1)\n",
    "\n",
    "X_test, X_val, Y_test, Y_val = train_test_split(\n",
    "    X_test, Y_test, test_size=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator found by grid search:\n",
      "\n",
      "Best parameters:  {'C': 100, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Mean Cross Validation Score: 0.62\n",
      "Training time: 40.251\n",
      "Accuracy on test set: 0.5217\n",
      "Accuracy on training set: 0.5552\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf_svm = getBestParamsForSVM(X_train,Y_train)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "clf_svm.fit(scaler.transform(X_train),Y_train)\n",
    "score = clf_svm.score(X_test,Y_test)\n",
    "score_train = clf_svm.score(X_train,Y_train)\n",
    "print(\"Accuracy on test set: {:.4f}\".format(score))\n",
    "print(\"Accuracy on training set: {:.4f}\".format(score_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridSearchCv for svm \n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000,5000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'kernel': ['rbf','linear'], 'class_weight': ['balanced']}\n",
    "grid = GridSearchCV(SVC(random_state=1), param_grid, cv=5, verbose=3)\n",
    "grid.fit(scaler.transform(X_train), Y_train)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "print(grid.score(scaler.transform(X_test), Y_test))\n",
    "print(grid.score(scaler.transform(X_train), Y_train))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "213524bb45a1aeaf737b1d8c77d7b8db5d425938d9dffc5f4bc6fe6dd3324700"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
