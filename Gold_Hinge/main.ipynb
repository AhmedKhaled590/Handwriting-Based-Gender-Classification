{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmath import e\n",
    "from time import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "# from skimage.feature import graycomatrix, graycoprops\n",
    "from skimage.measure import shannon_entropy as Entropy\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### program arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir=\"E:/CMP/NN/project\"\n",
    "hingeDir=dir+\"/Gold_Hinge/FeaturesOutput/hinge_features.npy\"\n",
    "goldDir=dir+\"/Gold_Hinge/FeaturesOutput/cold_features.npy\"\n",
    "icdarCold = dir+\"/Gold_Hinge/FeaturesOutput/icdar_cold_features.npy\"\n",
    "icdarHinge = dir+\"/Gold_Hinge/FeaturesOutput/icdar_hinge_features.npy\"\n",
    "icdarLabels = dir+\"/Gold_Hinge/FeaturesOutput/icdar_labels.npz\"\n",
    "labelsDir=dir+\"/Gold_Hinge/FeaturesOutput/labels.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hinge=np.load(hingeDir)\n",
    "X_gold=np.load(goldDir)\n",
    "Y=np.load(labelsDir)['label']\n",
    "label_names=np.load(labelsDir)['label_name']\n",
    "\n",
    "X_icdar_hinge = np.load(icdarHinge)\n",
    "X_icdar_cold = np.load(icdarCold)\n",
    "Y_icadar_labels = np.load(icdarLabels)['label']\n",
    "label_names_icdar = np.load(icdarLabels)['label_name']\n",
    "\n",
    "X_gold = np.concatenate((X_gold, X_icdar_cold), axis=0)\n",
    "X_hinge = np.concatenate((X_hinge, X_icdar_hinge), axis=0)\n",
    "\n",
    "\n",
    "# Y = np.concatenate((Y,Y_icadar_labels), axis=0) \n",
    "\n",
    "# label_names = np.concatenate((label_names,label_names_icdar), axis=0)\n",
    "\n",
    "# print(X_icdar_hinge.shape)\n",
    "# print(X_icdar_cold.shape)\n",
    "# print(Y_icadar_labels.shape)\n",
    "# print(label_names_icdar.shape)\n",
    "# print(Y_icadar_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_answers.csv')\n",
    "icdar_classes = df['male'].values\n",
    "icdar_labels = []\n",
    "for i in range(len(Y_icadar_labels)):\n",
    "    icdar_labels.append(icdar_classes[int(Y_icadar_labels[i])-1])\n",
    "Y = np.concatenate((Y,icdar_labels), axis=0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx=label_names=='icdarTrainImages'\n",
    "# Y[idx]=icdar_classes[int(Y[idx])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(741, 1200)\n",
      "(186, 1200)\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_test, Y_train, Y_test = train_test_split(X_hinge, Y, test_size=0.2, random_state=1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(np.concatenate((X_gold,X_hinge),axis=1), Y, test_size=0.2, random_state=1)\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(np.concatenate((X_icdar_cold,X_icdar_hinge),axis=1), icdar_labels, test_size=0.2, random_state=1)   \n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestParamsForSVM(X_train,Y_train,X_test,Y_test,outputFileName):\n",
    "    scaler = preprocessing.MinMaxScaler().fit(X_train)\n",
    "    C = [ 0.01, 0.1, 1, 10, 100,1000]\n",
    "    gamma = [0.0001,0.001, 0.01, 0.1, 1,'scale']\n",
    "    kernel = ['linear','rbf']\n",
    "    scores = []\n",
    "    scores_train = []\n",
    "    \n",
    "    with open(outputFileName,'w') as csvfile:\n",
    "        np.savetxt(csvfile, [], delimiter=',',\n",
    "                   header='C,gamma,kernel,score_train,score')\n",
    "    for i in range(len(C)):\n",
    "        for j in range(len(gamma)):\n",
    "            for k in range(len(kernel)):\n",
    "                clf = SVC(C=C[i], gamma=gamma[j],kernel=kernel[k],class_weight='balanced')\n",
    "                clf.fit(scaler.transform( X_train), Y_train)\n",
    "                temp_train = clf.score(scaler.transform(X_train), Y_train)\n",
    "                print(\"Accuracy on training set: {:.2f}\".format(\n",
    "                    temp_train))    \n",
    "                scores_train.append((temp_train,i,j))\n",
    "                temp = clf.score(scaler.transform( X_test), Y_test)\n",
    "                print(\"Accuracy on test set: {:.4f}\".format(\n",
    "                    temp))\n",
    "                scores.append((temp,i,j))\n",
    "                with open(outputFileName, 'a') as csvfile:\n",
    "                    np.savetxt(csvfile, np.array([[C[i],gamma[j],kernel[k],temp_train,temp]]), delimiter=',',fmt='%s')\n",
    "        \n",
    "    print(max(scores))\n",
    "    print(max(scores_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.73\n",
      "Accuracy on test set: 0.6613\n",
      "Accuracy on training set: 0.55\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.73\n",
      "Accuracy on test set: 0.6613\n",
      "Accuracy on training set: 0.55\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.73\n",
      "Accuracy on test set: 0.6613\n",
      "Accuracy on training set: 0.55\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.73\n",
      "Accuracy on test set: 0.6613\n",
      "Accuracy on training set: 0.55\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.73\n",
      "Accuracy on test set: 0.6613\n",
      "Accuracy on training set: 0.55\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.73\n",
      "Accuracy on test set: 0.6613\n",
      "Accuracy on training set: 0.55\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.86\n",
      "Accuracy on test set: 0.7043\n",
      "Accuracy on training set: 0.55\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.86\n",
      "Accuracy on test set: 0.7043\n",
      "Accuracy on training set: 0.55\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.86\n",
      "Accuracy on test set: 0.7043\n",
      "Accuracy on training set: 0.63\n",
      "Accuracy on test set: 0.5860\n",
      "Accuracy on training set: 0.86\n",
      "Accuracy on test set: 0.7043\n",
      "Accuracy on training set: 0.66\n",
      "Accuracy on test set: 0.6129\n",
      "Accuracy on training set: 0.86\n",
      "Accuracy on test set: 0.7043\n",
      "Accuracy on training set: 0.55\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.86\n",
      "Accuracy on test set: 0.7043\n",
      "Accuracy on training set: 0.66\n",
      "Accuracy on test set: 0.5860\n",
      "Accuracy on training set: 0.97\n",
      "Accuracy on test set: 0.7043\n",
      "Accuracy on training set: 0.45\n",
      "Accuracy on test set: 0.4301\n",
      "Accuracy on training set: 0.97\n",
      "Accuracy on test set: 0.7043\n",
      "Accuracy on training set: 0.64\n",
      "Accuracy on test set: 0.5968\n",
      "Accuracy on training set: 0.97\n",
      "Accuracy on test set: 0.7043\n",
      "Accuracy on training set: 0.78\n",
      "Accuracy on test set: 0.7097\n",
      "Accuracy on training set: 0.97\n",
      "Accuracy on test set: 0.7043\n",
      "Accuracy on training set: 0.96\n",
      "Accuracy on test set: 0.7473\n",
      "Accuracy on training set: 0.97\n",
      "Accuracy on test set: 0.7043\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.97\n",
      "Accuracy on test set: 0.7043\n",
      "Accuracy on training set: 0.86\n",
      "Accuracy on test set: 0.7419\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.7097\n",
      "Accuracy on training set: 0.65\n",
      "Accuracy on test set: 0.5914\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.7097\n",
      "Accuracy on training set: 0.78\n",
      "Accuracy on test set: 0.7258\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.7097\n",
      "Accuracy on training set: 0.94\n",
      "Accuracy on test set: 0.8118\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.7097\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.7957\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.7097\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.7097\n",
      "Accuracy on training set: 0.99\n",
      "Accuracy on test set: 0.8065\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.7097\n",
      "Accuracy on training set: 0.77\n",
      "Accuracy on test set: 0.7258\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.7097\n",
      "Accuracy on training set: 0.91\n",
      "Accuracy on test set: 0.7312\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.7097\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.7688\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.7097\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.7957\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.7097\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.7097\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.8118\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.7097\n",
      "Accuracy on training set: 0.90\n",
      "Accuracy on test set: 0.7097\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.7097\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.7043\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.7097\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.7849\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.7097\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.7957\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.7097\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.7097\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.8118\n",
      "(0.8118279569892473, 5, 5)\n",
      "(1.0, 5, 5)\n",
      "Accuracy on training set: 0.60\n",
      "Accuracy on test set: 0.5968\n",
      "Accuracy on training set: 0.55\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.60\n",
      "Accuracy on test set: 0.5968\n",
      "Accuracy on training set: 0.55\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.60\n",
      "Accuracy on test set: 0.5968\n",
      "Accuracy on training set: 0.55\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.60\n",
      "Accuracy on test set: 0.5968\n",
      "Accuracy on training set: 0.55\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.60\n",
      "Accuracy on test set: 0.5968\n",
      "Accuracy on training set: 0.55\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.60\n",
      "Accuracy on test set: 0.5968\n",
      "Accuracy on training set: 0.55\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.67\n",
      "Accuracy on test set: 0.6075\n",
      "Accuracy on training set: 0.55\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.67\n",
      "Accuracy on test set: 0.6075\n",
      "Accuracy on training set: 0.55\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.67\n",
      "Accuracy on test set: 0.6075\n",
      "Accuracy on training set: 0.58\n",
      "Accuracy on test set: 0.6022\n",
      "Accuracy on training set: 0.67\n",
      "Accuracy on test set: 0.6075\n",
      "Accuracy on training set: 0.66\n",
      "Accuracy on test set: 0.5968\n",
      "Accuracy on training set: 0.67\n",
      "Accuracy on test set: 0.6075\n",
      "Accuracy on training set: 0.86\n",
      "Accuracy on test set: 0.6344\n",
      "Accuracy on training set: 0.67\n",
      "Accuracy on test set: 0.6075\n",
      "Accuracy on training set: 0.63\n",
      "Accuracy on test set: 0.6075\n",
      "Accuracy on training set: 0.78\n",
      "Accuracy on test set: 0.5806\n",
      "Accuracy on training set: 0.45\n",
      "Accuracy on test set: 0.4301\n",
      "Accuracy on training set: 0.78\n",
      "Accuracy on test set: 0.5806\n",
      "Accuracy on training set: 0.58\n",
      "Accuracy on test set: 0.5914\n",
      "Accuracy on training set: 0.78\n",
      "Accuracy on test set: 0.5806\n",
      "Accuracy on training set: 0.64\n",
      "Accuracy on test set: 0.5968\n",
      "Accuracy on training set: 0.78\n",
      "Accuracy on test set: 0.5806\n",
      "Accuracy on training set: 0.78\n",
      "Accuracy on test set: 0.6559\n",
      "Accuracy on training set: 0.78\n",
      "Accuracy on test set: 0.5806\n",
      "Accuracy on training set: 0.96\n",
      "Accuracy on test set: 0.6667\n",
      "Accuracy on training set: 0.78\n",
      "Accuracy on test set: 0.5806\n",
      "Accuracy on training set: 0.74\n",
      "Accuracy on test set: 0.6505\n",
      "Accuracy on training set: 0.88\n",
      "Accuracy on test set: 0.5968\n",
      "Accuracy on training set: 0.57\n",
      "Accuracy on test set: 0.5860\n",
      "Accuracy on training set: 0.88\n",
      "Accuracy on test set: 0.5968\n",
      "Accuracy on training set: 0.64\n",
      "Accuracy on test set: 0.5806\n",
      "Accuracy on training set: 0.88\n",
      "Accuracy on test set: 0.5968\n",
      "Accuracy on training set: 0.74\n",
      "Accuracy on test set: 0.6290\n",
      "Accuracy on training set: 0.88\n",
      "Accuracy on test set: 0.5968\n",
      "Accuracy on training set: 0.93\n",
      "Accuracy on test set: 0.7043\n",
      "Accuracy on training set: 0.88\n",
      "Accuracy on test set: 0.5968\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6935\n",
      "Accuracy on training set: 0.88\n",
      "Accuracy on test set: 0.5968\n",
      "Accuracy on training set: 0.89\n",
      "Accuracy on test set: 0.6989\n",
      "Accuracy on training set: 0.96\n",
      "Accuracy on test set: 0.6075\n",
      "Accuracy on training set: 0.64\n",
      "Accuracy on test set: 0.5753\n",
      "Accuracy on training set: 0.96\n",
      "Accuracy on test set: 0.6075\n",
      "Accuracy on training set: 0.71\n",
      "Accuracy on test set: 0.6075\n",
      "Accuracy on training set: 0.96\n",
      "Accuracy on test set: 0.6075\n",
      "Accuracy on training set: 0.87\n",
      "Accuracy on test set: 0.6882\n",
      "Accuracy on training set: 0.96\n",
      "Accuracy on test set: 0.6075\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6075\n",
      "Accuracy on training set: 0.96\n",
      "Accuracy on test set: 0.6075\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6935\n",
      "Accuracy on training set: 0.96\n",
      "Accuracy on test set: 0.6075\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6183\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6183\n",
      "Accuracy on training set: 0.70\n",
      "Accuracy on test set: 0.5914\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6183\n",
      "Accuracy on training set: 0.83\n",
      "Accuracy on test set: 0.6344\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6183\n",
      "Accuracy on training set: 0.97\n",
      "Accuracy on test set: 0.6075\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6183\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.5914\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6183\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6935\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6183\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.5968\n",
      "(0.7043010752688172, 3, 3)\n",
      "(1.0, 5, 5)\n",
      "Accuracy on training set: 0.72\n",
      "Accuracy on test set: 0.6559\n",
      "Accuracy on training set: 0.55\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.72\n",
      "Accuracy on test set: 0.6559\n",
      "Accuracy on training set: 0.55\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.72\n",
      "Accuracy on test set: 0.6559\n",
      "Accuracy on training set: 0.55\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.72\n",
      "Accuracy on test set: 0.6559\n",
      "Accuracy on training set: 0.55\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.72\n",
      "Accuracy on test set: 0.6559\n",
      "Accuracy on training set: 0.55\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.72\n",
      "Accuracy on test set: 0.6559\n",
      "Accuracy on training set: 0.55\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.84\n",
      "Accuracy on test set: 0.7419\n",
      "Accuracy on training set: 0.55\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.84\n",
      "Accuracy on test set: 0.7419\n",
      "Accuracy on training set: 0.55\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.84\n",
      "Accuracy on test set: 0.7419\n",
      "Accuracy on training set: 0.62\n",
      "Accuracy on test set: 0.5645\n",
      "Accuracy on training set: 0.84\n",
      "Accuracy on test set: 0.7419\n",
      "Accuracy on training set: 0.68\n",
      "Accuracy on test set: 0.6344\n",
      "Accuracy on training set: 0.84\n",
      "Accuracy on test set: 0.7419\n",
      "Accuracy on training set: 0.55\n",
      "Accuracy on test set: 0.5699\n",
      "Accuracy on training set: 0.84\n",
      "Accuracy on test set: 0.7419\n",
      "Accuracy on training set: 0.64\n",
      "Accuracy on test set: 0.5914\n",
      "Accuracy on training set: 0.94\n",
      "Accuracy on test set: 0.7043\n",
      "Accuracy on training set: 0.45\n",
      "Accuracy on test set: 0.4301\n",
      "Accuracy on training set: 0.94\n",
      "Accuracy on test set: 0.7043\n",
      "Accuracy on training set: 0.61\n",
      "Accuracy on test set: 0.5591\n",
      "Accuracy on training set: 0.94\n",
      "Accuracy on test set: 0.7043\n",
      "Accuracy on training set: 0.75\n",
      "Accuracy on test set: 0.6613\n",
      "Accuracy on training set: 0.94\n",
      "Accuracy on test set: 0.7043\n",
      "Accuracy on training set: 0.95\n",
      "Accuracy on test set: 0.7527\n",
      "Accuracy on training set: 0.94\n",
      "Accuracy on test set: 0.7043\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.5806\n",
      "Accuracy on training set: 0.94\n",
      "Accuracy on test set: 0.7043\n",
      "Accuracy on training set: 0.88\n",
      "Accuracy on test set: 0.7688\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6505\n",
      "Accuracy on training set: 0.60\n",
      "Accuracy on test set: 0.5591\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6505\n",
      "Accuracy on training set: 0.74\n",
      "Accuracy on test set: 0.6828\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6505\n",
      "Accuracy on training set: 0.91\n",
      "Accuracy on test set: 0.8011\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6505\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.8280\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6505\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.5806\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6505\n",
      "Accuracy on training set: 0.99\n",
      "Accuracy on test set: 0.8333\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6452\n",
      "Accuracy on training set: 0.74\n",
      "Accuracy on test set: 0.6882\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6452\n",
      "Accuracy on training set: 0.88\n",
      "Accuracy on test set: 0.7527\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6452\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.7742\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6452\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.8280\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6452\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.5806\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6452\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.8226\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6452\n",
      "Accuracy on training set: 0.87\n",
      "Accuracy on test set: 0.7419\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6452\n",
      "Accuracy on training set: 0.98\n",
      "Accuracy on test set: 0.7419\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6452\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.7742\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6452\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.8280\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6452\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.5806\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.6452\n",
      "Accuracy on training set: 1.00\n",
      "Accuracy on test set: 0.8226\n",
      "(0.8333333333333334, 3, 5)\n",
      "(1.0, 5, 5)\n"
     ]
    }
   ],
   "source": [
    "getBestParamsForSVM(X_train,Y_train,X_test,Y_test,'svm_results.csv')\n",
    "getBestParamsForSVM(X_train[:,np.arange(0,421)],Y_train,X_test[:,np.arange(0,421)],Y_test,'svm_results_cold.csv')\n",
    "getBestParamsForSVM(X_train[:,np.arange(421,1200)],Y_train,X_test[:,np.arange(421,1200)],Y_test,'svm_results_hinge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier for gold features\n",
    "scaler = preprocessing.MinMaxScaler().fit(X_train[:,np.arange(0,421)])\n",
    "clf = SVC(C=1, gamma=0.001,kernel='linear',class_weight='balanced',random_state=1)\n",
    "X = scaler.transform(X_train[:,np.arange(0,421)])\n",
    "clf.fit(scaler.transform(X_train[:,np.arange(0,421)]), Y_train)\n",
    "df_gold = clf.decision_function(scaler.transform(X_test[:,np.arange(0,421)]))\n",
    "predictions_gold = clf.predict(scaler.transform(X_test[:,np.arange(0,421)]))\n",
    "score_test_gold = clf.score(scaler.transform( X_test[:,np.arange(0,421)]), Y_test)\n",
    "print(\"Accuracy on test set: {:.4f}\".format(\n",
    "    score_test_gold))\n",
    "score_train_gold = clf.score(scaler.transform( X_train[:,np.arange(0,421)]), Y_train)\n",
    "print(\"Accuracy on training set: {:.4f}\".format(\n",
    "    score_train_gold))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier for hinge features\n",
    "scaler = preprocessing.MinMaxScaler().fit(X_train[:,np.arange(421,1200)])\n",
    "clf = SVC(C=10, gamma='scale',kernel='rbf',class_weight='balanced',random_state=1)\n",
    "clf.fit(scaler.transform(X_train[:,np.arange(421,1200)]), Y_train)\n",
    "df_hinge = clf.decision_function(scaler.transform(X_test[:,np.arange(421,1200)]))\n",
    "predictions_hinge = clf.predict(scaler.transform(X_test[:,np.arange(421,1200)]))\n",
    "\n",
    "score_test_hinge = clf.score(scaler.transform( X_test[:,np.arange(421,1200)]), Y_test)\n",
    "print(\"Accuracy on test set: {:.4f}\".format(\n",
    "    score_test_hinge))\n",
    "score_train_hinge = clf.score(scaler.transform( X_train[:,np.arange(421,1200)]), Y_train)\n",
    "print(\"Accuracy on training set: {:.4f}\".format(\n",
    "    score_train_hinge))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the decision function\n",
    "df = []\n",
    "for i in range(len(df_gold)):\n",
    "    df.append(max(df_gold[i],df_hinge[i]))\n",
    "\n",
    "df = np.array(df)\n",
    "\n",
    "prediction_decision_function = np.where(df>=0,1,0)\n",
    "accuracy = accuracy_score(Y_test, prediction_decision_function)\n",
    "print(\"Accuracy on test set: {:.4f}\".format(\n",
    "    accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier for both\n",
    "scaler = preprocessing.MinMaxScaler().fit(X_train)\n",
    "clf = SVC(C=10, gamma='scale',kernel='rbf',class_weight='balanced',random_state=1)\n",
    "clf.fit(scaler.transform(X_train), Y_train)\n",
    "predictions_gold_hinge = clf.predict(scaler.transform(X_test))\n",
    "score_test_both = clf.score(scaler.transform( X_test), Y_test)\n",
    "print(\"Accuracy on test set: {:.4f}\".format(\n",
    "    score_test_both))\n",
    "score_train_both = clf.score(scaler.transform( X_train), Y_train)\n",
    "print(\"Accuracy on training set: {:.4f}\".format(\n",
    "    score_train_both))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting classifier\n",
    "final_predictions = []\n",
    "for i in range(len(predictions_gold)):\n",
    "    male = 0\n",
    "    female = 0\n",
    "    \n",
    "    female = (female+1) if predictions_gold[i]==0 else female\n",
    "    male = (male+1) if predictions_gold[i]==1 else male\n",
    "\n",
    "    female = (female+1) if predictions_hinge[i]==0 else female\n",
    "    male = (male+1) if predictions_hinge[i]==1 else male\n",
    "    \n",
    "    female = (female+1) if predictions_gold_hinge[i]==0 else female\n",
    "    male = (male+1) if predictions_gold_hinge[i]==1 else male\n",
    "\n",
    "\n",
    "    final_predictions.append(0 if female>male else 1)\n",
    "\n",
    "\n",
    "accuracy = sum(final_predictions==Y_test)/len(Y_test)\n",
    "print(\"Accuracy on test set: {:.4f}\".format(\n",
    "    accuracy))    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestParamsForRandomForest(X_train,Y_train,X_test,Y_test):\n",
    "    n_estimators = [10,50,100,200]\n",
    "    max_depth = [2,5,10,20,50]\n",
    "    min_samples_split = [2,5,10,20,50]\n",
    "    min_samples_leaf = [1,2,5,10,20]\n",
    "    max_features = ['auto']\n",
    "    bootstrap = [True,False]\n",
    "    scores = []\n",
    "    scores_train = []\n",
    "    with open('randomForest0.csv', 'w') as csvfile:\n",
    "        np.savetxt(csvfile,[],\n",
    "                   header='n_estimators,max_depth,min_samples_split,min_samples_leaf,max_features,bootstrap,score_training,score',\n",
    "                   delimiter=',', fmt='%s')\n",
    "    for i in range(len(n_estimators)):\n",
    "        for j in range(len(max_depth)):\n",
    "            for k in range(len(min_samples_split)):\n",
    "                for l in range(len(min_samples_leaf)):\n",
    "                    for m in range(len(max_features)):\n",
    "                        for n in range(len(bootstrap)):\n",
    "                                clf = RandomForestClassifier(n_estimators=n_estimators[i],max_depth=max_depth[j],min_samples_split=min_samples_split[k],\n",
    "                                                             min_samples_leaf=min_samples_leaf[l],max_features=max_features[m],bootstrap=bootstrap[n],\n",
    "                                                             criterion='gini',random_state=1)\n",
    "                                clf.fit(X_train, Y_train)\n",
    "                                temp_train = clf.score(X_train, Y_train)\n",
    "                                scores_train.append((temp_train,i,j,k,l,m,n))\n",
    "                                print(\"Accuracy on training set: {:.2f}\".format(\n",
    "                                    temp_train))\n",
    "                                temp = clf.score( X_test, Y_test)\n",
    "                                scores.append((temp,i,j,k,l,m,n))\n",
    "                                print(\"Accuracy on test set: {:.4f}\".format(\n",
    "                                    temp))\n",
    "                                print(\"\\n\")\n",
    "                                with open('randomForest0.csv', 'a') as csvFile:\n",
    "                                    np.savetxt(csvFile, np.array([[n_estimators[i],max_depth[j],min_samples_split[k],min_samples_leaf[l],max_features[m],bootstrap[n],temp_train,temp]]), delimiter=',', fmt='%s')\n",
    "    print(max(scores))\n",
    "    print(max(scores_train))\n",
    "# getBestParamsForRandomForest(X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestParamsForDT(X_train,Y_train,X_test,Y_test):\n",
    "    max_depth = [2,5,10,20,50,100]\n",
    "    min_samples_split = [2,5,10,20,50]\n",
    "    min_samples_leaf = [1,2,5,10,20]\n",
    "    max_features = ['auto','sqrt','log2']\n",
    "    scores = []\n",
    "    scores_train = []\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    with open('decisionTree.csv', 'w') as csvfile:\n",
    "        np.savetxt(csvfile,[],\n",
    "                   header='max_depth,min_samples_split,min_samples_leaf,max_features,score_training,score',\n",
    "                   delimiter=',', fmt='%s')\n",
    "    for i in range(len(max_depth)):\n",
    "        for j in range(len(min_samples_split)):\n",
    "            for k in range(len(min_samples_leaf)):\n",
    "                for l in range(len(max_features)):\n",
    "                        clf = DecisionTreeClassifier(random_state=1,max_depth=max_depth[i],min_samples_split=min_samples_split[j],\n",
    "                                                     min_samples_leaf=min_samples_leaf[k],max_features=max_features[l],criterion='gini')\n",
    "                        clf.fit(scaler.transform(X_train), Y_train)\n",
    "                        temp_train = clf.score(scaler.transform( X_train), Y_train)\n",
    "                        scores_train.append((temp_train,i,j,k,l))\n",
    "                        print(\"Accuracy on training set: {:.2f}\".format(\n",
    "                            temp_train))\n",
    "                        temp = clf.score(scaler.transform( X_test), Y_test)\n",
    "                        scores.append((temp,i,j,k,l))\n",
    "                        print(\"Accuracy on test set: {:.4f}\".format(\n",
    "                            temp))\n",
    "                        print(\"\\n\")\n",
    "                        with open('decisionTree.csv', 'a') as csvFile:\n",
    "                            np.savetxt(csvFile, np.array([[max_depth[i],min_samples_split[j],min_samples_leaf[k],max_features[l],temp_train,temp]]), delimiter=',', fmt='%s')\n",
    "\n",
    "getBestParamsForDT(X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = preprocessing.MinMaxScaler().fit(X_train)\n",
    "X_train = cs.transform(X_train) \n",
    "X_train = np.nan_to_num(X_train)\n",
    "\n",
    "X_test = cs.transform(X_test) \n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "clf = SVC(kernel='rbf', verbose=True, C=10)\n",
    "clf.fit(X_train, Y_train)\n",
    "# print(clf.predict(X_test))\n",
    "print(clf.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = preprocessing.MinMaxScaler()\n",
    "X = cs.fit_transform(X_hinge) \n",
    "X = np.nan_to_num(X)\n",
    "clf = SVC(kernel='rbf', verbose=True, C=10)\n",
    "scores = cross_val_score(clf, X, Y, cv=10)\n",
    "print(scores)\n",
    "y_pred_hinge = cross_val_predict(clf, X, Y, cv=10)\n",
    "# print(y_pred_hinge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = preprocessing.MinMaxScaler()\n",
    "X = cs.fit_transform(X_gold)\n",
    "X = np.nan_to_num(X)\n",
    "clf = SVC(kernel='rbf', verbose=True, C=10)\n",
    "scores = cross_val_score(clf, X, Y, cv=10)\n",
    "print(scores)\n",
    "y_pred_cold = cross_val_predict(clf, X, Y, cv=10)\n",
    "print(y_pred_cold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.maximum(y_pred_hinge, y_pred_cold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(y_pred_hinge == Y) / float(len(Y)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "213524bb45a1aeaf737b1d8c77d7b8db5d425938d9dffc5f4bc6fe6dd3324700"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
